{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "unet.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arthurbabey/road66/blob/arthur/gpu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sPCdsZZqymCk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import keras\n",
        "import numpy as np\n",
        "#import tensorflow as tf\n",
        "import os,sys\n",
        "%matplotlib inline\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "from skimage.transform import rotate, resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras import backend as keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3ySRehUiWX2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "528fd9d8-bf70-46f9-b5aa-d694b56f0259"
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3xr6WVH7pZ7",
        "colab_type": "code",
        "outputId": "e182414a-aa2a-4a2b-e404-80d71cee184b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        }
      },
      "source": [
        "\"\"\"\n",
        "Use 1.x tensorflow version\n",
        "\"\"\"\n",
        "\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SystemError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mSystemError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-fcb7df5f2c0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgpu_device_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdevice_name\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'/device:GPU:0'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mSystemError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'GPU device not found'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found GPU at: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mSystemError\u001b[0m: GPU device not found"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbF0z9006-L_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "This cells contains all the function needed\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "def load_image(infilename):\n",
        "    data = mpimg.imread(infilename)\n",
        "    return data\n",
        "\n",
        "# Concatenate an image and its groundtruth\n",
        "def concatenate_images(img, gt_img):\n",
        "    nChannels = len(gt_img.shape)\n",
        "    w = gt_img.shape[0]\n",
        "    h = gt_img.shape[1]\n",
        "    if nChannels == 3:\n",
        "        cimg = np.concatenate((img, gt_img), axis=1)\n",
        "    else:\n",
        "        gt_img_3c = np.zeros((w, h, 3), dtype=np.uint8)\n",
        "        gt_img8 = img_float_to_uint8(gt_img)\n",
        "        gt_img_3c[:,:,0] = gt_img8\n",
        "        gt_img_3c[:,:,1] = gt_img8\n",
        "        gt_img_3c[:,:,2] = gt_img8\n",
        "        img8 = img_float_to_uint8(img)\n",
        "        cimg = np.concatenate((img8, gt_img_3c), axis=1)\n",
        "    return cimg\n",
        "\n",
        "# Convert array of labels to an image\n",
        "\n",
        "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
        "    im = np.zeros([imgwidth, imgheight])\n",
        "    idx = 0\n",
        "    for i in range(0,imgheight,h):\n",
        "        for j in range(0,imgwidth,w):\n",
        "            im[j:j+w, i:i+h] = labels[idx]\n",
        "            idx = idx + 1\n",
        "    return im\n",
        "\n",
        "def value_to_class(v, foreground_threshold=0.25):\n",
        "\n",
        "    df = np.sum(v)\n",
        "    if df > foreground_threshold:\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def rotate_images(X, Y , degrees):\n",
        "    \"\"\"\n",
        "    increase the number of data\n",
        "    by adding rotations of the base data\n",
        "    \"\"\"\n",
        "\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "    rotimg = np.zeros(X.shape)\n",
        "    rotgtimg = np.zeros(Y.shape)\n",
        "\n",
        "    Xtemp = X\n",
        "    Ytemp = Y\n",
        "\n",
        "    #rotate all images by degree and add them to the data vector\n",
        "    for degree in degrees:\n",
        "        for i in range(len(Xtemp)):\n",
        "            rotimg[i] = rotate(Xtemp[i], degree, resize=False, mode='reflect')\n",
        "            rotgtimg[i] = rotate(Ytemp[i], degree, resize=False, mode='reflect')\n",
        "        X = np.concatenate([X,rotimg])\n",
        "        Y = np.concatenate([Y,rotgtimg])\n",
        "\n",
        "    return X,Y\n",
        "\n",
        "def resize_image(X, Y, size = 400):\n",
        "\n",
        "    X = np.asarray(X)\n",
        "    Y = np.asarray(Y)\n",
        "\n",
        "    Xresize = np.asarray([resize(X[i], (size,size), mode = 'reflect') for i in range(X.shape[0])])\n",
        "    Yresize = np.asarray([resize(Y[i], (size,size), mode = 'reflect') for i in range(X.shape[0])])\n",
        "\n",
        "    return Xresize, Yresize\n",
        "\n",
        "\n",
        "def create_submission(y_pred, filename = 'filename', patch_size = 16, img_size = 608):\n",
        "    \"\"\"\n",
        "    Create a submission (csv format) for AIcrowd\n",
        "    from given prediction\n",
        "\n",
        "    \"\"\"\n",
        "    n = img_size // patch_size\n",
        "    y_pred = np.reshape(y_pred, (-1, n, n))\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        f.write('id,prediction\\n')\n",
        "        for i in range(y_pred.shape[0]):\n",
        "            img = y_pred[i]\n",
        "            for j in range(img.shape[0]):\n",
        "                for k in range(img.shape[1]):\n",
        "                    name = '{:03d}_{}_{},{}'.format(i + 1, j * patch_size, k * patch_size, int(img[j,k]))\n",
        "                    f.write(name + '\\n')\n",
        "\n",
        "\n",
        "def img_crop(im, w, h, border = 0, step = 16):\n",
        "    \"\"\"\n",
        "    Return the patches list of an image.\n",
        "    \"\"\"\n",
        "    list_patches = []\n",
        "    imgwidth = im.shape[0]\n",
        "    imgheight = im.shape[1]\n",
        "    is_2d = len(im.shape) < 3\n",
        "    if border != 0:\n",
        "        im_r = np.pad(im[:,:,0], ((border, border), (border, border)), 'reflect')\n",
        "        im_g = np.pad(im[:,:,1], ((border, border), (border, border)), 'reflect')\n",
        "        im_b = np.pad(im[:,:,2], ((border, border), (border, border)), 'reflect')\n",
        "        im = np.dstack((im_r, im_g, im_b))\n",
        "    for i in range(0,imgheight,step):\n",
        "        for j in range(0,imgwidth,step):\n",
        "            if is_2d:\n",
        "                im_patch = im[j:j+w+2*border, i:i+h+2*border]\n",
        "            else:\n",
        "                im_patch = im[j:j+w+2*border, i:i+h+2*border, :]\n",
        "            list_patches.append(im_patch)\n",
        "    return list_patches\n",
        "\n",
        "\n",
        "def load_trainset(path = 'Data/test_set_images'):\n",
        "\n",
        "    # Loaded a set of images\n",
        "    root_dir = path\n",
        "\n",
        "    image_dir = root_dir + \"images/\"\n",
        "    files = os.listdir(image_dir)\n",
        "    n = len(files) # Use all images\n",
        "    print(\"Loading \" + str(n) + \" testing images\")\n",
        "    imgs = [load_image(image_dir + files[i]) for i in range(100)]\n",
        "\n",
        "    gt_dir = root_dir + \"groundtruth/\"\n",
        "    print(\"Loading \" + str(n) + \" groundtruth\")\n",
        "    gt_imgs = [load_image(gt_dir + files[i]) for i in range(100)]\n",
        "\n",
        "    return imgs, gt_imgs\n",
        "\n",
        "\n",
        "def load_testset(path = 'Data/test_set_images'):\n",
        "\n",
        "\n",
        "  root_testdir = path\n",
        "  test_names = os.listdir(root_testdir)\n",
        "\n",
        "  num_test = len(test_names)\n",
        "  order = [int(test_names[i].split(\"_\")[1]) for i in range(num_test)]\n",
        "  index = np.argsort(order)\n",
        "\n",
        "  # Load image and reorder them\n",
        "  imgs_test = [load_image(os.path.join(root_testdir, test_names[i], test_names[i]) + \".png\")\n",
        "               for i in range(num_test)]\n",
        "  imgs_test = [imgs_test[i] for i in index]\n",
        "\n",
        "  return imgs_test\n",
        "\n",
        "def XYaugmentGenerator(X1, y, gen, seed = 1, batch_size = 1):\n",
        "    genX1 = gen.flow(X1, y, batch_size=batch_size, seed=seed)\n",
        "    genX2 = gen.flow(y, X1, batch_size=batch_size, seed=seed)\n",
        "    while True:\n",
        "        X1i = genX1.next()\n",
        "        X2i = genX2.next()\n",
        "\n",
        "        yield X1i[0], X2i[0]\n",
        "\n",
        "def test_image_unet_submission(imgs_test, model, size = 400, foreground_threshold = 0.25, filename = 'submission.csv'):\n",
        "\n",
        "\n",
        "  img1 = []\n",
        "  img2 = []\n",
        "  img3 = []\n",
        "  img4 = []\n",
        "\n",
        "  shift = 608 - size\n",
        "\n",
        "  for img in imgs_test:\n",
        "    img = img[0:size, 0:size, :]\n",
        "    img1.append(img)\n",
        "\n",
        "  for img in imgs_test:\n",
        "    img = img[shift:608, 0:size, :]\n",
        "    img2.append(img)\n",
        "\n",
        "  for img in imgs_test:\n",
        "    img = img[0:size, shift:608, :]\n",
        "    img3.append(img)\n",
        "\n",
        "  for img in imgs_test:\n",
        "    img = img[shift:608, shift:608, :]\n",
        "    img4.append(img)\n",
        "\n",
        "  img_pred1 = model.predict(np.asarray(img1), batch_size = 1, verbose = 1)\n",
        "  img_pred2 = model.predict(np.asarray(img2), batch_size = 1, verbose = 1)\n",
        "  img_pred3 = model.predict(np.asarray(img3), batch_size = 1, verbose = 1)\n",
        "  img_pred4 = model.predict(np.asarray(img4), batch_size = 1, verbose = 1)\n",
        "\n",
        "\n",
        "  img_pred1[img_pred1 <= 0.5] = 0\n",
        "  img_pred1[img_pred1 > 0.5] = 1\n",
        "\n",
        "  img_pred2[img_pred2 <= 0.5] = 0\n",
        "  img_pred2[img_pred2 > 0.5] = 1\n",
        "\n",
        "  img_pred3[img_pred3 <= 0.5] = 0\n",
        "  img_pred3[img_pred3 > 0.5] = 1\n",
        "\n",
        "  img_pred4[img_pred4 <= 0.5] = 0\n",
        "  img_pred4[img_pred4 > 0.5] = 1\n",
        "\n",
        "  img1 = np.asarray(img1)\n",
        "  img2 = np.asarray(img2)\n",
        "  img3 = np.asarray(img3)\n",
        "  img4 = np.asarray(img4)\n",
        "\n",
        "  list_m1 = []\n",
        "  list_m2 = []\n",
        "  list_merge = []\n",
        "\n",
        "  for i in range(50):\n",
        "    m1 = np.concatenate((img_pred1[i, 0:shift, :, :], img_pred2[i, :, :, :]), axis = 0)\n",
        "    list_m1.append(m1)\n",
        "    m2 = np.concatenate((img_pred3[i, 0:shift, :, :], img_pred4[i, :, :, :]), axis = 0)\n",
        "    list_m2.append(m2)\n",
        "\n",
        "  list_m1 = np.asarray(list_m1)\n",
        "  list_m2 = np.asarray(list_m2)\n",
        "\n",
        "  for i in range(50):\n",
        "    merge = np.concatenate((list_m1[i,:,:,:], list_m2[i, :, (size - shift):size, :]), axis=1)\n",
        "    list_merge.append(merge)\n",
        "\n",
        "  y_pred = np.asarray(list_merge)\n",
        "\n",
        "  pred_patch = [img_crop(y_pred[i], 16, 16) for i in range(y_pred.shape[0])]\n",
        "  pred_patch = np.asarray([pred_patch[i][j] for i in range(len(pred_patch)) for j in range(len(pred_patch[i]))])\n",
        "  pred_patch = np.asarray([value_to_class(np.mean(pred_patch[i]), foreground_threshold=foreground_threshold) for i in range(pred_patch.shape[0])])\n",
        "\n",
        "  create_submission(pred_patch, filename)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGFWJndu5hJg",
        "colab_type": "code",
        "outputId": "04d7754b-089f-4b6d-ec40-fa973dfa55a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnDfgued5y_C",
        "colab_type": "code",
        "outputId": "888af79b-d4c0-4e95-ee46-2e2712ccce7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "imgs, gt_imgs = load_trainset(path = 'drive/My Drive/training/')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading 100 testing images\n",
            "Loading 100 groundtruth\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tzwcZ5WodFvF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5432dd28-23c8-4315-aee6-e3dc7771ba0a"
      },
      "source": [
        "root_testdir = \"drive/My Drive/test_set_images\"\n",
        "test_names = os.listdir(root_testdir)\n",
        "\n",
        "num_test = len(test_names)\n",
        "order = [int(test_names[i].split(\"_\")[1]) for i in range(num_test)]\n",
        "index = np.argsort(order)\n",
        "\n",
        "imgs_test = [load_image(os.path.join(root_testdir, test_names[i], test_names[i]) + \".png\") \n",
        "             for i in range(num_test)]\n",
        "imgs_test = [imgs_test[i] for i in index]\n",
        "print('Test set loaded')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set loaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5k4sd2N68r9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imgs, gt_imgs = rotate_images(imgs, gt_imgs, [ 15, 30, 45, 60, 75])\n",
        "\n",
        "n = len(imgs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aAQNH5BJVsH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEED = 2019\n",
        "np.random.seed(SEED)\n",
        "tf.compat.v1.set_random_seed(SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijCDcdBWCy3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_size = 400\n",
        "\n",
        "Xr, Yr = resize_image(imgs, gt_imgs, img_size)\n",
        "Yr = np.reshape(Yr, (n, img_size, img_size, 1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5Z6roEC7GDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train, x_val, y_train, y_val = train_test_split(Xr, Yr, test_size=0.1,  random_state=SEED)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIT775u2Yxpa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unet(input_size = (400,400,3)):\n",
        "    inputs = Input(input_size)\n",
        "    conv1 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(inputs)\n",
        "    conv1 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "    conv2 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool1)\n",
        "    conv2 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv2)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "    conv3 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool2)\n",
        "    conv3 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool3)\n",
        "    conv4 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv4)\n",
        "    drop4 = Dropout(0.25)(conv4)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(drop4)\n",
        "\n",
        "    conv5 = Conv2D(2048, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(pool4)\n",
        "    conv5 = Conv2D(2048, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv5)\n",
        "    drop5 = Dropout(0.5)(conv5)\n",
        "\n",
        "    up6 = Conv2D(1024, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(drop5))\n",
        "    merge6 = concatenate([drop4,up6], axis = 3)\n",
        "    conv6 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge6)\n",
        "    conv6 = Conv2D(1024, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv6)\n",
        "\n",
        "    up7 = Conv2D(512, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge7)\n",
        "    conv7 = Conv2D(512, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv7)\n",
        "\n",
        "    up8 = Conv2D(256, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv7))\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge8)\n",
        "    conv8 = Conv2D(256, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv8)\n",
        "\n",
        "    up9 = Conv2D(128, 2, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(merge9)\n",
        "    conv9 = Conv2D(128, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv9 = Conv2D(2, 3, activation = 'relu', padding = 'same', kernel_initializer = 'he_normal')(conv9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "\n",
        "    model = Model(inputs, conv10)\n",
        "\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ua8eMgndzfM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = (400, 400, 3)\n",
        "model = unet((400, 400, 3))\n",
        "\n",
        "#model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOP8E8URdhC-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 378
        },
        "outputId": "8fb36f7c-f41e-4639-ba56-3dc3b3929747"
      },
      "source": [
        "pip install --upgrade keras"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\r\u001b[K     |▉                               | 10kB 30.4MB/s eta 0:00:01\r\u001b[K     |█▊                              | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |███▌                            | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |████▍                           | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████                         | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 378kB 2.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.9.0 in /tensorflow-2.1.0/python3.6 (from keras) (1.13.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.3)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /tensorflow-2.1.0/python3.6 (from keras) (1.17.4)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /tensorflow-2.1.0/python3.6 (from keras) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /tensorflow-2.1.0/python3.6 (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /tensorflow-2.1.0/python3.6 (from keras) (2.10.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_whMllLLeSQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "a5a7d3d1-d2c1-449d-8de3-0beb5d90d68e"
      },
      "source": [
        "import keras\n",
        "keras.__version__"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.3.1'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vG-5u8X8CjBv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create image generator\n",
        "data_gen_args = dict(\n",
        "        width_shift_range=0.05,\n",
        "        height_shift_range=0.05,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        vertical_flip=True,)\n",
        "\n",
        "\n",
        "image_datagen = ImageDataGenerator(**data_gen_args, fill_mode ='reflect')\n",
        "\n",
        "train_gen = XYaugmentGenerator(x_train, y_train, image_datagen)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fPuFEbmGn6c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_filename = 'model_name.h5'\n",
        "\n",
        "callback_checkpoint = ModelCheckpoint(model_filename, verbose=1, monitor='val_loss', save_best_only=True)\n",
        "lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=7, verbose=1, mode='min', min_lr= 1e-8)\n",
        "es = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtUtxfGmCkna",
        "colab_type": "code",
        "outputId": "b766e235-3ff7-48bb-ab50-a8bd79004de0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        }
      },
      "source": [
        "model.compile(loss='binary_crossentropy', \n",
        "              optimizer= Adam(lr = 1e-4),\n",
        "              metrics=['binary_accuracy'])\n",
        "\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=540,\n",
        "    epochs=120,   \n",
        "    validation_data=(x_val, y_val),\n",
        "    callbacks=[callback_checkpoint, es, lr]\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 540 steps, validate on 60 samples\n",
            "Epoch 1/120\n",
            "  6/540 [..............................] - ETA: 4:35:35 - loss: 0.5872 - binary_accuracy: 0.7179WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy\n",
            "WARNING:tensorflow:Reduce LR on plateau conditioned on metric `val_loss` which is not available. Available metrics are: loss,binary_accuracy,lr\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-eb7b6ccfd6d6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m )\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    485\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNJb-ZotEl8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save_weights(filepath='drive/My Drive/model_name.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iv1OEx0ycjSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Loaded our best model, the model can be downloaded from the link in the readme and has to be\n",
        "upload in your drive\n",
        "\"\"\"\n",
        "model.load_weights('drive/My Drive/lastrun.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4M0Vcz4KhSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Run this cell if you want to plot loss/val_loss from the previous training\n",
        "\"\"\"\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sa6h1rBMe6Hp",
        "colab_type": "code",
        "outputId": "eac700e0-ba23-4e5e-d93f-513add3aed28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "test_image_unet_submission(imgs_test, model, filename = 'drive/My Drive/submission.csv', foreground_threshold = 0.25)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50/50 [==============================] - 6s 128ms/step\n",
            "50/50 [==============================] - 6s 128ms/step\n",
            "50/50 [==============================] - 6s 128ms/step\n",
            "50/50 [==============================] - 6s 128ms/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xs11xHltHA9H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}